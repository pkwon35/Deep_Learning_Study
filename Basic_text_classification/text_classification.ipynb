{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic text classification\n",
    "\n",
    "## 참고 : https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "\n",
    "### [2022년 6월10일 ~ 6월18일]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# 영화 리뷰를 사용한 텍스트 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 예시에서는 영화 리뷰(후기) 텍스트를 가져와서 이 후기는  \"긍정\" (positive) 또는 \"부정\" (negative)로 분류 해볼꺼에요.\n",
    "\n",
    "이 예제는 이진(binary, 두개로 분류) 즉 예측을 둘중 하나의 값으로 분류하는 문제입니다.  예) 긍정 / 부정, 사람이다 / 아니다, 등등 두개의 값으로 많이 사용되는 분류방법입니다.\n",
    "\n",
    "해당 예시에서는 tf.keras 모듈을 사용해서 해볼려고 합니다!\n",
    "\n",
    "그러기 위해서 일단 필요한 모듈을 설치 및 불러와 봐요~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg62Pmz3o83v"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 모듈 설치 및 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해당 예시에서는 tensorflow  2.8.0 버전을 사용할것입니다.\n",
    "\n",
    "#### 아래 불러오기 함수가 실행이 안되면 아래 pip 코드 실행하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==\"2.8.0\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 GPU 지정    https://velog.io/@jaeha0725/%ED%8A%B9%EC%A0%95-GPU-%EC%A7%80%EC%A0%95-os.environCUDAVISIBLEDEVICES\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:40.138081Z",
     "iopub.status.busy": "2020-09-23T07:21:40.137445Z",
     "iopub.status.idle": "2020-09-23T07:21:46.446713Z",
     "shell.execute_reply": "2020-09-23T07:21:46.447247Z"
    },
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# 필요 모듈 불러오기\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAsKG535pHep"
   },
   "source": [
    "## IMDB 데이터셋 다운로드\n",
    "\n",
    "IMDB 데이터셋은 텐서플로우에서 쉽게 불러와 사용할 수 있도록 미리 전처리 된 형태에서 제공하고 있어요.\n",
    "\n",
    "어떤 전처리를 해두었을까요?\n",
    "\n",
    "- string에서 숫자로 변환\n",
    "    - 우리가 보통 생각하기에 리뷰 데이터는 \"와 이 영화 진짜 개꿀잼\" 이런식으로 string 형식이겠죠. 하지만, 우리가 사용할 모델은 이런 string 형식이 input으로 들어오면 이해하지 못해요, 왜냐하면 모델은 \"숫자\"를 받아 프로세스 하기 때문이죠!\n",
    "\n",
    "- 숫자로 변환? 근데 어떻게??\n",
    "    - 주어진 string 형식의 정보를 변환해 모델에 넣어주는 방식은 여러가지가 있어요! 해당 예시에서 우리가 변환하는 방식은 간단해요!\n",
    "        - 특정 단어가 유일한 숫자값으로 mapping 되어있는 어휘사전으로 각 string을 숫자로 변화했어요!\n",
    "        - 조금 더 풀어서 얘기하면.. 일단 이번에 분석하고자하는 모든 데이터에 존재하는 모든 unique 단어를 찾아서 각각 다른 숫자를 지정해서 dictionary로 만들어주면 되요!\n",
    "             \n",
    "             {'I': 100,  'tsukino': 52006,  'nunnery': 52007,  'love': 16816, 'vani': 63951, 'woods': 1408, 'spiders': 16115, ... }\n",
    " \n",
    "        - 위와 같은 dictionary가 만들어지고, \"I love spiders\" 를 string -> 숫자로 변환한다면   100,16816,16115    이런식으로 바꿔서 데이터를 사용할 수 있겠죠?\n",
    "\n",
    "#### 아래에서 keras.datasets.imdb 데이터를 불러와봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:46.451714Z",
     "iopub.status.busy": "2020-09-23T07:21:46.451075Z",
     "iopub.status.idle": "2020-09-23T07:21:51.511011Z",
     "shell.execute_reply": "2020-09-23T07:21:51.510277Z"
    },
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb   # keras에 있는 많은 datasets 예시에서 imdb 데이터 셋 object 불러오기\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)     \n",
    "# load_data 함수 사용해서 imdb 데이터의 train / test   데이터와 레이블 각각 불러오기     \n",
    "# num_words = 10000 을 설정하면 전체 데이터넷에서 많이 나타난 unique 한 단어 상위 10000개를 가져와요. 만약 100이라 하면 unique 100개만 가져오겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "word_index = imdb.get_word_index()\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words 100 하면,   0~99로 각 단어를 변환한다. 즉 max는 99. (그렇다고 각 문서의 가장 큰 값이 99는 아니다. 해당 단어가 해당 리뷰에 없으면 더 작은값이 나옴)\n",
    "\n",
    "(train_data_100, train_labels_100), (test_data_100, test_labels_100) = imdb.load_data(num_words=100)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< num_words = 100 설정했을때 >\n",
      "각 문서에서 가장 큰 값\n",
      "[1, 99, 78, 13, 66, 40, 2, 2, 5, 2, 2, 21, 4, 2, 2, 8, 2, 14, 2, 2, 6, 2, 7, 2, 6, 2, 7, 2, 21, 12, 32, 2, 2, 2, 4, 2, 2, 81, 27, 2, 2, 2, 2, 2, 2, 38, 76, 2, 2, 11, 6, 2, 2, 8, 2, 4, 22, 14, 2, 7, 4, 2, 2, 2, 12, 17, 2, 2, 2, 2, 4, 58, 8, 2, 12, 2, 2, 62, 28, 2, 6, 2, 2]\n",
      "\n",
      "모든 문서에서 가장 큰 값\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(\"< num_words = 100 설정했을때 >\")\n",
    "print(\"각 문서에서 가장 큰 값\")\n",
    "print(max(train_data_100))\n",
    "print()\n",
    "print(\"모든 문서에서 가장 큰 값\")\n",
    "print(max(max(train_data_100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odr-KlzO-lkL"
   },
   "source": [
    "`num_words=10000`은 훈련 데이터에서 가장 많이 등장하는 상위 10,000개의 단어를 선택합니다.\n",
    "\n",
    "데이터 크기를 적당하게 유지하기 위해 드물게 등장하는 단어는 제외하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## 데이터 탐색\n",
    "\n",
    "이 데이터셋의 샘플은 전처리된 정수 배열입니다.\n",
    "\n",
    "    - 이 정수는 영화 리뷰에 나오는 단어를 나타냅니다.\n",
    "\n",
    "레이블(label)은 정수 0 또는 1입니다.\n",
    "\n",
    "    - 0은 부정적인 리뷰이고 1은 긍정적인 리뷰입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnKvHWW4-lkW"
   },
   "source": [
    "리뷰 텍스트는 어휘 사전의 특정 단어를 나타내는 정수로 변환되어 있습니다. 첫 번째 리뷰를 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:51.522456Z",
     "iopub.status.busy": "2020-09-23T07:21:51.521790Z",
     "iopub.status.idle": "2020-09-23T07:21:51.524067Z",
     "shell.execute_reply": "2020-09-23T07:21:51.524506Z"
    },
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "\n",
      "unique 한 단어만 확인\n",
      "[   1    2    4    5    6    7    8    9   12   13   14   15   16   17\n",
      "   18   19   21   22   25   26   28   30   32   33   35   36   38   39\n",
      "   43   46   48   50   51   52   56   62   65   66   71   76   77   82\n",
      "   87   88   92   98  100  103  104  106  107  112  113  117  124  130\n",
      "  134  135  141  144  147  150  167  172  173  178  192  194  215  224\n",
      "  226  256  283  284  297  316  317  336  381  385  386  400  407  447\n",
      "  458  469  476  480  515  530  546  619  626  670  723  838  973 1029\n",
      " 1111 1247 1334 1385 1415 1622 1920 2025 2071 2223 3766 3785 3941 4468\n",
      " 4472 4536 4613 5244 5345 5535 5952 7486]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print()\n",
    "print(\"unique 한 단어만 확인\")\n",
    "print(np.unique(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 학습을 진행하기전, 해당 리뷰의 정체를 먼저 확인해봐요!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n",
      "88584\n"
     ]
    }
   ],
   "source": [
    "# word_index 확인하기\n",
    "\n",
    "a=list(word_index.values())\n",
    "\n",
    "a.sort()\n",
    "\n",
    "print(max(a))\n",
    "print(len(a))\n",
    "\n",
    "# 확인해보니 연속적으로 0 ~ 88590 까지 있는 것은 아니다. 2개 정도 비어있는 것으로 확인됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 불러온 각 단어와 숫자가 매칭되어있는 word_index 딕셔너리를 활용해 변환을 해보겠습니다\n",
    "word_index = imdb.get_word_index()\n",
    "len(word_index)\n",
    "\n",
    "\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}   # 해당 예시에서는 word의 0 , 1 , 2 , 3 으로 <PAD> , <START> , <UNK> ,  <UNUSED>  추가해야합니다.\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])    # 기존 키값으로 단어가 들어있던 딘셔너리를    키값으로 숫자로    키값과 value값을 스위치 해주는 코드에요  예)  {\"I\":10 , \"love\" : 90}  -->  {10 : \"I\" , 90 : \"love\"}\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])    # 여기서 딕셔너리.get() 함수를 이해해보아요!!   dictionary.get(찾고자하는key값, 해당키값 없으면 돌려주는 value)  로 이해하면되요. \n",
    "                                                                        # 즉, reverse_word_index.get(i, '?') 에서   만약 reverse_word_index 안에 i값이 있으면, 해당 value를 돌려주지만, 만약 해당 i값이 존재하지 않으면 ? 를 돌려줍니다.\n",
    "\n",
    "\n",
    "decode_review(train_data[0])\n",
    "\n",
    "\n",
    "# 아래 결과에서 보이는 <UNK> 의 경우, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFP_XKVRp4_S"
   },
   "source": [
    "## 데이터 준비\n",
    "\n",
    "\n",
    "위에서 string을 숫자로 변환하였지만, 신경망에 주입하기 전 몇가지 변환을 더 해줘야합니다!\n",
    "\n",
    "\n",
    "- 리뷰(정수) 리스트형식의 배열은 신경망에 주입하기 전에 텐서로 변환해야합니다!\n",
    "\n",
    "해당 변환 방법에는 몇 가지가 있습니다:\n",
    "\n",
    "\n",
    "- ### 1) 원-핫 인코딩(one-hot encoding)은 정수 배열을 0과 1로 이루어진 벡터로 변환합니다. 예를 들어 배열 [3, 5]을 인덱스 3과 5만 1이고 나머지는 모두 0인 10,000차원 벡터로 변환할 수 있습니다. 그다음 실수 벡터 데이터를 다룰 수 있는 층-Dense 층-을 신경망의 첫 번째 층으로 사용합니다. 이 방법은 `num_words * num_reviews` 크기의 행렬이 필요하기 때문에 메모리를 많이 사용합니다.\n",
    "\n",
    "\n",
    "- ### 2) 정수 배열의 길이가 모두 같도록 패딩(padding)을 추가해 `max_length * num_reviews` 크기의 정수 텐서를 만듭니다. 이런 형태의 텐서를 다룰 수 있는 임베딩(embedding) 층을 신경망의 첫 번째 층으로 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "해당 예시에서는 두번째 방법인 \"정수 배열의 길이가 같도록 패딩(padding)을 추가하겠습니다!\n",
    "\n",
    "\n",
    "영화 리뷰의 길이가 같아야 하므로 [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) 함수를 사용해 길이를 맞추겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 450 129\n"
     ]
    }
   ],
   "source": [
    "# padding을 적용하기전 데이터의 길이를 비교해보겠습니다\n",
    "\n",
    "print(len(train_data[0]),len(train_data[10]),len(train_data[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:51.901032Z",
     "iopub.status.busy": "2020-09-23T07:21:51.900342Z",
     "iopub.status.idle": "2020-09-23T07:21:52.915360Z",
     "shell.execute_reply": "2020-09-23T07:21:52.914520Z"
    },
    "id": "2jQv-omsHurp"
   },
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],  ## 아까 word_index 에서 \"<PAD>\"\" 로 지정한 숫자, 0을 추가하도록 하겠습니다\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO5MBpyQdipD"
   },
   "source": [
    "샘플의 길이를 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:52.920484Z",
     "iopub.status.busy": "2020-09-23T07:21:52.919787Z",
     "iopub.status.idle": "2020-09-23T07:21:52.922925Z",
     "shell.execute_reply": "2020-09-23T07:21:52.922417Z"
    },
    "id": "USSSBnkE-lky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256 256\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]),len(train_data[10]),len(train_data[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJoxZGyfjT5V"
   },
   "source": [
    "(패딩된) 첫 번째 리뷰 내용을 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:52.927715Z",
     "iopub.status.busy": "2020-09-23T07:21:52.927036Z",
     "iopub.status.idle": "2020-09-23T07:21:52.929853Z",
     "shell.execute_reply": "2020-09-23T07:21:52.929354Z"
    },
    "id": "TG8X9cqi-lk9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "\n",
    "# 보이는 것과 같이 원래 218 였던 길아의 데이터에 0이 추가로 들어간 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## 모델 구성\n",
    "\n",
    "이제 데이터도 모델에 들어갈 준비가 완료됬습니다!!\n",
    "\n",
    "\n",
    "이제 데이터를 넣어줄 모델을 쌓아보죠.\n",
    "\n",
    "\n",
    "신경망은 층(layer)을 쌓아서 만듭니다. 이 구조에서는 두 가지를 결정해야 합니다:\n",
    "\n",
    "1) 모델에서 얼마나 많은 층을 사용할 것인가?\n",
    "2) 각 층에서 얼마나 많은 은닉 유닛(hidden unit)을 사용할 것인가?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 이 예제의 입력 데이터는 단어 인덱스의 배열입니다.\n",
    "- 예측할 레이블은 0 또는 1입니다. (binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:52.935695Z",
     "iopub.status.busy": "2020-09-23T07:21:52.934804Z",
     "iopub.status.idle": "2020-09-23T07:21:54.626430Z",
     "shell.execute_reply": "2020-09-23T07:21:54.626871Z"
    },
    "id": "xpKOoWgu-llD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 18:39:27.890866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:27.920260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:27.922107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:27.927336: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-28 18:39:27.931080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:27.932499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:27.934037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:30.410182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:30.411647: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:30.412965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-28 18:39:30.415345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:00:06.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# 입력 크기는 영화 리뷰 데이터셋에 적용된 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))   # 해당 레이어를 지나면, 각 단어에서 context를 뽑아내는 과정을 거칩니다. \n",
    "model.add(keras.layers.GlobalAveragePooling1D())                         # Pooling에 대한 개념을 알고 있으면 좋을 것입니다.   https://gaussian37.github.io/dl-concept-global_average_pooling/\n",
    "                                                                        # 그냥 average pooling 1D 개녕 : https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/average-pooling-1d \n",
    "                                                                        # globalAveragePooling1D 개념 : https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/global-average-pooling-1d\n",
    "                                                                        \n",
    "model.add(keras.layers.Dense(16, activation='relu'))                    # relu 함수의 개념 : https://gooopy.tistory.com/55#:~:text=%EB%A0%90%EB%A3%A8%20%ED%95%A8%EC%88%98(Rectified%20Linear%20Unit%2C%20ReLU)&text=%EB%A0%90%EB%A3%A8%20%ED%95%A8%EC%88%98%EB%8A%94%20%EC%9A%B0%EB%A6%AC%20%EB%A7%90%EB%A1%9C,%EC%9D%84%20%EC%B0%A8%EB%8B%A8%ED%95%9C%EB%8B%A4%EB%8A%94%20%EC%9D%98%EB%AF%B8%EB%8B%A4.\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PbKQ6mucuKL"
   },
   "source": [
    "층을 순서대로 쌓아 분류기(classifier)를 만듭니다:\n",
    "\n",
    "1. 첫 번째 층은 `Embedding` 층입니다. 이 층은 정수로 인코딩된 단어를 입력 받고 각 단어 인덱스에 해당하는 임베딩 벡터를 찾습니다. 이 벡터는 모델이 훈련되면서 학습됩니다. 이 벡터는 출력 배열에 새로운 차원으로 추가됩니다. 최종 차원은 `(batch, sequence, embedding)`이 됩니다.\n",
    "\n",
    "2. 그다음 `GlobalAveragePooling1D` 층은 `sequence` 차원에 대해 평균을 계산하여 각 샘플에 대해 고정된 길이의 출력 벡터를 반환합니다. 이는 길이가 다른 입력을 다루는 가장 간단한 방법입니다. https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/global-average-pooling-1d\n",
    "\n",
    "3. 이 고정 길이의 출력 벡터는 16개의 은닉 유닛을 가진 완전 연결(fully-connected) 층(`Dense`)을 거칩니다.\n",
    "\n",
    "4. 마지막 층은 하나의 출력 노드(node)를 가진 완전 연결 층입니다. `sigmoid` 활성화 함수를 사용하여 0과 1 사이의 실수를 출력합니다. 이 값은 확률 또는 신뢰도를 나타냅니다.\n",
    "\n",
    "\n",
    "\n",
    "### 위에서 모델을 쌓아주었으니, 이제 해당 모델의 학습을 위해 컴파일 하겠습니다. (컴파일이란 모델을 학습시키기 위한 학습과정을 설정하는 단계)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### 컴파일 함수로 \"손실 함수\"와 \"옵티마이저\" 설정하기\n",
    "\n",
    "모델이 훈련하려면 손실 함수(loss function)과 옵티마이저(optimizer)가 필요합니다. 이 예제는 이진 분류 문제이고 모델이 확률을 출력하므로(출력층의 유닛이 하나이고 `sigmoid` 활성화 함수를 사용합니다), `binary_crossentropy` 손실 함수를 사용하겠습니다.\n",
    "\n",
    "binary_crossentropy : 확률 분포 간의 거리를 측정하는 함수. 정답인 타깃 분포와 예측 분포 사이의 거리를 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:54.639995Z",
     "iopub.status.busy": "2020-09-23T07:21:54.639227Z",
     "iopub.status.idle": "2020-09-23T07:21:54.647130Z",
     "shell.execute_reply": "2020-09-23T07:21:54.646492Z"
    },
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCWYwkug-llQ"
   },
   "source": [
    "## 검증 세트 만들기\n",
    "\n",
    "모델링을 할때 주어진 데이터를 3가지로 나눠서 보통 진행합니다.\n",
    "\n",
    "- 학습 (train)\n",
    "- 검증 (validation)\n",
    "- 테스트 (test)\n",
    "\n",
    "\n",
    "모델을 훈련하면서 \"학습\" 데이터와 \"검증\" 데이터를 사용합니다. 다만, 모델 학습시 가중치에 영향을 주는 데이터는 오직 \"학습\" 데이터 입니다. 검증 데이터는 단순히 모델이 훈련되면서 해당 모델에 아무런 영향은 주지 않은 검증데이터의 정확도를 확인하기위해 활용됩니다.\n",
    "\n",
    "- 중요) 모델 \"학습\" 데이터를 사용해 학습하며 \"검증\" 데이터로 정확도를 확인하지만, 얼마나 검증 데이터의 정확도를 잘 측정하는지는 학습과정에 아무런 영향을 주지 않습니다. \n",
    "\n",
    "- 그럼 검증데이터와 테스트 데이터는 비슷한 역활을 하는 것일까요?\n",
    "    - 훈련 데이터만을 사용하여 모델을 개발하고 튜닝하는 것이 목표입니다! 이후 테스트 세트를 사용해서 딱 한 번만 정확도를 평가합니다\n",
    "\n",
    "\n",
    "\n",
    "### 원본 훈련 데이터에서 10,000개의 샘플을 떼어내어 검증 데이터 (validation set)를 만들어봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:54.651865Z",
     "iopub.status.busy": "2020-09-23T07:21:54.651213Z",
     "iopub.status.idle": "2020-09-23T07:21:54.653218Z",
     "shell.execute_reply": "2020-09-23T07:21:54.653608Z"
    },
    "id": "-NpcXY9--llS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]\n",
    "\n",
    "print(len(x_val))  # 검증 10000개\n",
    "print(len(partial_x_train))  # 학습 15000개\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## 모델 훈련\n",
    "### 참고 : https://m.blog.naver.com/qbxlvnf11/221449297033\n",
    "모델을 훈련하기위해 설정해줘야하는 여러가지 옵션이 있습니다!\n",
    "\n",
    "\n",
    "\n",
    "- epochs\n",
    "    - 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것을 말함. 즉, 전체 데이터 셋에 대해 한 번 학습을 완료한 상태\n",
    "    - 우리는 모델을 만들 때 적절한 epoch 값을 설정해야만 underfitting과 overfitting을 방지할 수 있습니다.\n",
    "    - epoch 값이 너무 작다면 underfitting이 너무 크다면 overfitting이 발생할 확률이 높은 것이죠.\n",
    "\n",
    "\n",
    "- batch_size\n",
    "    - batch size는 한 번의 batch마다 주는 데이터 샘플의 size. 여기서 batch(보통 mini-batch라고 표현)는 나눠진 데이터 셋을 뜻하며 iteration는 epoch를 나누어서 실행하는 횟수라고 생각하면 됨\n",
    "    - 메모리의 한계와 속도 저하 때문에 대부분의 경우에는 한 번의 epoch에서 모든 데이터를 한꺼번에 집어넣을 수는 없습니다. 그래서 데이터를 나누어서\n",
    "\n",
    "\n",
    "![image](https://mblogthumb-phinf.pstatic.net/MjAxOTAxMjNfMjU4/MDAxNTQ4MjM1Nzg3NTA2.UtvnGsckZhLHOPPOBWH841IWsZFzNcgwZvYKi2nxImEg.CdtqIxOjWeBo4eNBD2pXu5uwYGa3ZVUr8WZvtldArtYg.PNG.qbxlvnf11/20190123_182720.png?type=w800)\n",
    "\n",
    "\n",
    "- verbose\n",
    "    - 학습시 프린트되는 과정을 설정\n",
    "    - verbose: Integer. 0, 1, or 2. \n",
    "    - 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "\n",
    "\n",
    "- validation_date\n",
    "    - 설정시 학습시 검증 데이터를 모델로 검증한 결과를 프린트 같이 해줌\n",
    "\n",
    "\n",
    "등등 여러가지가 있습니다. \n",
    "\n",
    "이 모델을 512개의 샘플로 이루어진 미니배치(mini-batch)에서 40번의 에포크(epoch) 동안 훈련합니다. `x_train`과 `y_train` 텐서에 있는 모든 샘플에 대해 40번 반복한다는 뜻입니다. 훈련하는 동안 10,000개의 검증 세트에서 모델의 손실과 정확도를 모니터링합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:54.658876Z",
     "iopub.status.busy": "2020-09-23T07:21:54.658250Z",
     "iopub.status.idle": "2020-09-23T07:22:08.874375Z",
     "shell.execute_reply": "2020-09-23T07:22:08.874803Z"
    },
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "11/30 [==========>...................] - ETA: 0s - loss: 0.6930 - accuracy: 0.5158  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-28 18:39:39.324783: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 4s 12ms/step - loss: 0.6920 - accuracy: 0.5737 - val_loss: 0.6899 - val_accuracy: 0.6673\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6860 - accuracy: 0.7099 - val_loss: 0.6816 - val_accuracy: 0.7202\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6729 - accuracy: 0.7549 - val_loss: 0.6644 - val_accuracy: 0.7434\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6485 - accuracy: 0.7769 - val_loss: 0.6364 - val_accuracy: 0.7651\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6124 - accuracy: 0.7917 - val_loss: 0.5986 - val_accuracy: 0.7865\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5669 - accuracy: 0.8120 - val_loss: 0.5549 - val_accuracy: 0.8047\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5170 - accuracy: 0.8309 - val_loss: 0.5093 - val_accuracy: 0.8204\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4679 - accuracy: 0.8479 - val_loss: 0.4678 - val_accuracy: 0.8343\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4233 - accuracy: 0.8626 - val_loss: 0.4308 - val_accuracy: 0.8436\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3854 - accuracy: 0.8729 - val_loss: 0.4009 - val_accuracy: 0.8533\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3532 - accuracy: 0.8834 - val_loss: 0.3779 - val_accuracy: 0.8590\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3273 - accuracy: 0.8899 - val_loss: 0.3587 - val_accuracy: 0.8629\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3045 - accuracy: 0.8958 - val_loss: 0.3434 - val_accuracy: 0.8693\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2855 - accuracy: 0.9027 - val_loss: 0.3323 - val_accuracy: 0.8698\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2687 - accuracy: 0.9069 - val_loss: 0.3227 - val_accuracy: 0.8734\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2539 - accuracy: 0.9115 - val_loss: 0.3138 - val_accuracy: 0.8779\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2403 - accuracy: 0.9165 - val_loss: 0.3071 - val_accuracy: 0.8807\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2287 - accuracy: 0.9203 - val_loss: 0.3020 - val_accuracy: 0.8812\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2172 - accuracy: 0.9255 - val_loss: 0.2974 - val_accuracy: 0.8831\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2071 - accuracy: 0.9279 - val_loss: 0.2940 - val_accuracy: 0.8823\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1979 - accuracy: 0.9323 - val_loss: 0.2913 - val_accuracy: 0.8842\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1888 - accuracy: 0.9366 - val_loss: 0.2890 - val_accuracy: 0.8846\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1803 - accuracy: 0.9408 - val_loss: 0.2874 - val_accuracy: 0.8856\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1725 - accuracy: 0.9437 - val_loss: 0.2866 - val_accuracy: 0.8842\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1651 - accuracy: 0.9476 - val_loss: 0.2861 - val_accuracy: 0.8847\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1584 - accuracy: 0.9499 - val_loss: 0.2866 - val_accuracy: 0.8848\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1518 - accuracy: 0.9533 - val_loss: 0.2867 - val_accuracy: 0.8849\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1458 - accuracy: 0.9552 - val_loss: 0.2872 - val_accuracy: 0.8855\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1398 - accuracy: 0.9575 - val_loss: 0.2880 - val_accuracy: 0.8855\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1341 - accuracy: 0.9605 - val_loss: 0.2894 - val_accuracy: 0.8840\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1290 - accuracy: 0.9633 - val_loss: 0.2903 - val_accuracy: 0.8851\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1238 - accuracy: 0.9647 - val_loss: 0.2939 - val_accuracy: 0.8849\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1189 - accuracy: 0.9662 - val_loss: 0.2943 - val_accuracy: 0.8849\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1140 - accuracy: 0.9683 - val_loss: 0.2975 - val_accuracy: 0.8842\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1099 - accuracy: 0.9692 - val_loss: 0.2993 - val_accuracy: 0.8836\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1054 - accuracy: 0.9711 - val_loss: 0.3016 - val_accuracy: 0.8833\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1016 - accuracy: 0.9724 - val_loss: 0.3048 - val_accuracy: 0.8841\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0975 - accuracy: 0.9742 - val_loss: 0.3081 - val_accuracy: 0.8821\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0940 - accuracy: 0.9755 - val_loss: 0.3118 - val_accuracy: 0.8832\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0902 - accuracy: 0.9767 - val_loss: 0.3143 - val_accuracy: 0.8816\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,  # 전체 데이터를 40번 사용해서 학습을 거치는 것입니다\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## 모델 평가\n",
    "\n",
    "모델의 성능을 확인해 보죠. 두 개의 값이 반환됩니다. 손실(오차를 나타내는 숫자이므로 낮을수록 좋습니다)과 정확도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  591,  202, ...,    0,    0,    0],\n",
       "       [   6,  176,    7, ...,  125,    4, 3077],\n",
       "       [  57, 4893,    5, ...,    9,   57,  975],\n",
       "       ...,\n",
       "       [   1,   13, 1408, ...,    0,    0,    0],\n",
       "       [   1,   11,  119, ...,    0,    0,    0],\n",
       "       [   1,    6,   52, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:22:08.879756Z",
     "iopub.status.busy": "2020-09-23T07:22:08.879054Z",
     "iopub.status.idle": "2020-09-23T07:22:10.410627Z",
     "shell.execute_reply": "2020-09-23T07:22:10.411068Z"
    },
    "id": "zOMKywn4zReN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 1s - loss: 0.3343 - accuracy: 0.8719 - 1s/epoch - 2ms/step\n",
      "[0.3343137204647064, 0.8718799948692322]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data,  test_labels, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## 정확도와 손실 그래프 그리기\n",
    "\n",
    "`model.fit()`은 `History` 객체를 반환합니다. 여기에는 훈련하는 동안 일어난 모든 정보가 담긴 딕셔너리(dictionary)가 들어 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:22:10.415929Z",
     "iopub.status.busy": "2020-09-23T07:22:10.415051Z",
     "iopub.status.idle": "2020-09-23T07:22:10.417656Z",
     "shell.execute_reply": "2020-09-23T07:22:10.418079Z"
    },
    "id": "VcvSXvhp-llb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하며 각 epoch에서의 결과를 쌓아둔 것이다\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6919686794281006, 0.6860054731369019, 0.6728731393814087, 0.6484948992729187, 0.6124134063720703, 0.5669257044792175, 0.5169719457626343, 0.46794214844703674, 0.42334499955177307, 0.3853691518306732, 0.35324206948280334, 0.32733988761901855, 0.30452778935432434, 0.2854941785335541, 0.26872676610946655, 0.25389015674591064, 0.24029886722564697, 0.22870811820030212, 0.21724271774291992, 0.20708757638931274, 0.19789260625839233, 0.18875525891780853, 0.18028411269187927, 0.1725493222475052, 0.1650768220424652, 0.15843579173088074, 0.1517782360315323, 0.14576564729213715, 0.13976246118545532, 0.1340809315443039, 0.12901435792446136, 0.123766228556633, 0.11888451874256134, 0.11395356804132462, 0.10987076163291931, 0.10543777048587799, 0.1016039177775383, 0.0975375548005104, 0.09399417042732239, 0.09023696929216385]\n",
      "\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(history_dict['loss'])\n",
    "print()\n",
    "print(len(history_dict['loss']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRKsqL40-lle"
   },
   "source": [
    "네 개의 항목이 있습니다. 훈련과 검증 단계에서 모니터링하는 지표들입니다. 훈련 손실과 검증 손실을 그래프로 그려 보고, 훈련 정확도와 검증 정확도도 그래프로 그려서 비교해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:22:10.424038Z",
     "iopub.status.busy": "2020-09-23T07:22:10.423349Z",
     "iopub.status.idle": "2020-09-23T07:22:10.732376Z",
     "shell.execute_reply": "2020-09-23T07:22:10.732967Z"
    },
    "id": "nGoYf2Js-lle"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 41)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "print(epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:22:10.749404Z",
     "iopub.status.busy": "2020-09-23T07:22:10.742226Z",
     "iopub.status.idle": "2020-09-23T07:22:10.887415Z",
     "shell.execute_reply": "2020-09-23T07:22:10.888043Z"
    },
    "id": "6hXx-xOv-llh"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/keras/text_classification#exercise_multi-class_classification_on_stack_overflow_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 예시 : multi-class (다중클래스) 분류 on Stack Overflow questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 진행한 binary 예시를 참고해서 stack overflow 질문 자료를 다중클래스 학습에 적용해봐요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download dataset\n",
    "# %%capture\n",
    "# !gzip -d /home/jkwon/tasks/Basic_text_classification/stack_overflow_16k.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !tar -xvf /home/jkwon/tasks/Basic_text_classification/stack_overflow_16k.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고1 : https://www.gcptutorials.com/post/how-to-use-text_dataset_from_directory-in-tensorflow\n",
    "# 참고2 : https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\n",
    "# 참고3 : https://www.tensorflow.org/tutorials/load_data/text\n",
    "batch_size=52\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터셋 만들기\n",
    "raw_train_ds=utils.text_dataset_from_directory(\n",
    "    \"/home/jkwon/tasks/Basic_text_classification/train\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    batch_size=batch_size,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    follow_links=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 검증 데이터셋 만들기\n",
    "raw_val_ds = utils.text_dataset_from_directory(\n",
    "    \"/home/jkwon/tasks/Basic_text_classification/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to csharp\n",
      "Label 1 corresponds to java\n",
      "Label 2 corresponds to javascript\n",
      "Label 3 corresponds to python\n"
     ]
    }
   ],
   "source": [
    "# 4개의 레이블 확인\n",
    "\n",
    "for i, label in enumerate(raw_train_ds.class_names):\n",
    "    print(\"Label\", i, \"corresponds to\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review b\"how to generate a unique 4-digit string i'm looking for a way to generate a (fairly) unique (non auto-incrementing) 4-digit string using the numbers 0 - 9 for each digit using blank.  i can validate uniqueness and generate another number if a dup is found.  i had thought about basing the number somehow on the datetime object's ticks property but am having a difficult time putting the pieces together...any thoughts or expertise would be much appreciated.\\n\"\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(1):\n",
    "    print(\"Review\", text_batch.numpy()[i])\n",
    "    print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋 만들기\n",
    "raw_test_ds = utils.text_dataset_from_directory(\n",
    "    \"/home/jkwon/tasks/Basic_text_classification/test\",\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 과정에서 데이터셋을 각 용도에 따라 나누어주었습니다. 하지만, 모델에 넣기위해 아직 남은 과정이 있습니다!\n",
    "\n",
    "1) standardization\n",
    "    - 문자를 전처리하는 과정\n",
    "        - remove punctuation / html elements\n",
    "\n",
    "2) tokenization\n",
    "    - 각 단어를 토큰화, 특정 단어에서 같은 의미지만 다른 형식으로 변형된 단어를 동일시 취급하여 unique 한 단어 수를 줄이기 위한 과정\n",
    "\n",
    "3) vectorization\n",
    "    - 각 단어를 숫자로 변형 및 벡터화\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 과정은 'tf.keras.layers.TextVectorization' 함수로 가능합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어의 빈도를 사용한 벡터 생성\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary') # output_mode 에서 'binary' 를 사용하면 bag-of-words 모델을 만드는 것이다 : Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return binary_vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question tf.Tensor(b'\"blank not working on any internet explorer version i am a newbie programmer but i was assign a project and i am having trouble getting this code to work on internet explorer. it works on firefox, chrome and safari.  in the developer console in ie, it keeps saying s1 is undefined.  i found the blank code on stack\\'s overflow.  basically what i want is if the user answer yes to any of the question, it will redirect them to page, and if they answer no to all the questions it will redirect them to a different page...        &lt;div&gt;.          &lt;label&gt; a. are you a programmer?&lt;/label&gt;.          &lt;select id=\"\"s1\"\" name=\"\"menu\"\" onchange=\"\"gotopage(this)\"\"&gt;.            &lt;option value=\"\"#\"\"&gt;select&lt;/option&gt;.            &lt;option value=\"\"yes\"\"&gt;yes&lt;/option&gt;.            &lt;option value=\"\"no\"\"&gt;no&lt;/option&gt;.          &lt;/select&gt;.        &lt;/div&gt;.        &lt;div&gt;.          &lt;label&gt; b. are you over 18?&lt;/label&gt;.          &lt;select id=\"\"s2\"\" name=\"\"menu\"\" onchange=\"\"gotopage(this)\"\"&gt;.            &lt;option value=\"\"#\"\"&gt;select&lt;/option&gt;.            &lt;option value=\"\"yes\"\"&gt;yes&lt;/option&gt;.            &lt;option value=\"\"no\"\"&gt;no&lt;/option&gt;.          &lt;/select&gt;.        &lt;/div&gt;.        &lt;div&gt;.          &lt;label&gt; c. do you like apples?&lt;/label&gt;            .          &lt;select id=\"\"s3\"\" name=\"\"menu\"\" onchange=\"\"gotopage(this)\"\"&gt;.            &lt;option value=\"\"#\"\"&gt;select&lt;/option&gt;.            &lt;option value=\"\"yes\"\"&gt;yes&lt;/option&gt;.            &lt;option value=\"\"no\"\"&gt;no&lt;/option&gt;.          &lt;/select&gt;.        &lt;/div&gt;    ...&lt;script&gt;..  function gotopage(){..  if(s1.value == \"\"yes\"\" || s2.value == \"\"yes\"\" || s3.value == \"\"yes\"\") {.    window.location = \"\"http://www.yahoo.com\"\";.  } else if(s1.value == \"\"no\"\" &amp;&amp; s2.value == \"\"no\"\" &amp;&amp; s3.value == \"\"no\"\") {.    window.location = \"\"http://www.google.com\"\";.  } .}..&lt;/script&gt;...i also tried the code on this page get selected value of option with multiple dropdown menus using blank. can someone direct me to the correct solution or give me a hint? sorry for the bad coding.\"\\n', shape=(), dtype=string)\n",
      "\n",
      "Label tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a batch (of 32 reviews and labels) from the dataset.\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_question, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Question\", first_question)\n",
    "print()\n",
    "print(\"Label\", first_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'binary' vectorized question: tf.Tensor([[1. 1. 1. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"'binary' vectorized question:\",\n",
    "      binary_vectorize_text(first_question, first_label)[0])\n",
    "\n",
    "## 아래는 10000개의 단어에 대해서 나타난 횟수를 보여주는 bag of words 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"1289 ---> \", int_vectorize_layer.get_vocabulary()[1289])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
    "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
    "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? 질문\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "124/124 [==============================] - 2s 13ms/step - loss: 1.1731 - accuracy: 0.6227 - val_loss: 1.0025 - val_accuracy: 0.7306\n",
      "Epoch 2/10\n",
      "124/124 [==============================] - 0s 4ms/step - loss: 0.8547 - accuracy: 0.8184 - val_loss: 0.8430 - val_accuracy: 0.7569\n",
      "Epoch 3/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.6994 - accuracy: 0.8564 - val_loss: 0.7551 - val_accuracy: 0.7750\n",
      "Epoch 4/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.6011 - accuracy: 0.8798 - val_loss: 0.6986 - val_accuracy: 0.7844\n",
      "Epoch 5/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.8975 - val_loss: 0.6590 - val_accuracy: 0.7900\n",
      "Epoch 6/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.4774 - accuracy: 0.9136 - val_loss: 0.6297 - val_accuracy: 0.7912\n",
      "Epoch 7/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.4345 - accuracy: 0.9233 - val_loss: 0.6073 - val_accuracy: 0.7950\n",
      "Epoch 8/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.9298 - val_loss: 0.5896 - val_accuracy: 0.7944\n",
      "Epoch 9/10\n",
      "124/124 [==============================] - 0s 4ms/step - loss: 0.3690 - accuracy: 0.9367 - val_loss: 0.5755 - val_accuracy: 0.7987\n",
      "Epoch 10/10\n",
      "124/124 [==============================] - 0s 4ms/step - loss: 0.3431 - accuracy: 0.9419 - val_loss: 0.5640 - val_accuracy: 0.7975\n"
     ]
    }
   ],
   "source": [
    "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
    "\n",
    "binary_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = binary_model.fit(\n",
    "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 0s 3ms/step - loss: 0.5491 - accuracy: 0.8076\n",
      "0.8076249957084656\n",
      "0.5491493344306946\n"
     ]
    }
   ],
   "source": [
    "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)\n",
    "print(binary_accuracy)\n",
    "print(binary_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorized layer를 사용한 1D ConvNet 결과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels):\n",
    "  model = tf.keras.Sequential([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels):\n",
    "  model = tf.keras.Sequential([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Dense(16, activation='relu'),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'int' vectorized question: tf.Tensor(\n",
      "[[  16   23  158   37   75 1257 2157  456    3   34    5 1618 2044   26\n",
      "     3  120  604    5  262    8    3   34  230  584  151   13   28    4\n",
      "   138   37 1257 2157   11  182   37 1529  841    8 2526    7    2 2008\n",
      "   335    7  434   11 1097  861 1372    6  436    3  225    2   16   28\n",
      "    37 7217 2361  637   54    3   44    6   10    2  104  216  628    4\n",
      "    75    9    2  159   11   72 1644  184    4  215    8   10  208  216\n",
      "   136    4   73    2  775   11   72 1644  184    4    5  178  215  530\n",
      "  3624    5   61   64    5    1 2366    1    1    1  599 7010  599    1\n",
      "   599    1 3867  530  530 3624  113   61   64  317    1 2366    1    1\n",
      "     1  599 7010  599    1  599    1 3867  530  530 3624  142   41   64\n",
      "    48    1 2366    1    1    1  599 7010  599    1  599    1 3867  530\n",
      "   895   38    1    1  628    1  628    1  628 2705    1   49    1  136\n",
      "   144    1  136  144    1  136 2705 7778    1  174  145    2   28   37\n",
      "    13  215   40  491   51    9  536   22  260 1115 4553   47   16   35\n",
      "   289 2677   74    4    2  247  290   46  381   74    5 2378  784   12\n",
      "     2  901  907    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 250), dtype=int64)\n",
      "313 --->  long\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "# 숫자 형식으로 변환해주는 과정\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "int_vectorize_layer.adapt(train_text)\n",
    "\n",
    "def int_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return int_vectorize_layer(text), label\n",
    "\n",
    "\n",
    "print(\"'int' vectorized question:\",\n",
    "      int_vectorize_text(first_question, first_label)[0])\n",
    "\n",
    "## 동일 250 길이의 벡터를 위해 padding 추가되어 있음\n",
    "\n",
    "print(\"313 ---> \", int_vectorize_layer.get_vocabulary()[313])\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))\n",
    "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
    "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
    "int_test_ds = raw_test_ds.map(int_vectorize_text)\n",
    "int_train_ds = configure_dataset(int_train_ds)\n",
    "int_val_ds = configure_dataset(int_val_ds)\n",
    "int_test_ds = configure_dataset(int_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# 입력 크기는 영화 리뷰 데이터셋에 적용된 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "vocab_size = 10000\n",
    "\n",
    "int_model = keras.Sequential()\n",
    "int_model.add(keras.layers.Embedding(vocab_size, 64, input_shape=(None,)))   # 해당 레이어를 지나면, 각 단어에서 context를 뽑아내는 과정을 거칩니다. \n",
    "int_model.add(keras.layers.GlobalAveragePooling1D())                         # Pooling에 대한 개념을 알고 있으면 좋을 것입니다.   https://gaussian37.github.io/dl-concept-global_average_pooling/\n",
    "                                                                        # 그냥 average pooling 1D 개녕 : https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/average-pooling-1d \n",
    "                                                                        # globalAveragePooling1D 개념 : https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/global-average-pooling-1d\n",
    "                                                                        \n",
    "int_model.add(keras.layers.Dense(16, activation='relu'))                    # relu 함수의 개념 : https://gooopy.tistory.com/55#:~:text=%EB%A0%90%EB%A3%A8%20%ED%95%A8%EC%88%98(Rectified%20Linear%20Unit%2C%20ReLU)&text=%EB%A0%90%EB%A3%A8%20%ED%95%A8%EC%88%98%EB%8A%94%20%EC%9A%B0%EB%A6%AC%20%EB%A7%90%EB%A1%9C,%EC%9D%84%20%EC%B0%A8%EB%8B%A8%ED%95%9C%EB%8B%A4%EB%8A%94%20%EC%9D%98%EB%AF%B8%EB%8B%A4.\n",
    "\n",
    "int_model.add(keras.layers.Dense(1, activation='sigmoid'))                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          640000    \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                1040      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 641,057\n",
      "Trainable params: 641,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "int_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 1.2212 - accuracy: 0.5170 - val_loss: 0.9848 - val_accuracy: 0.6675\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.7688 - accuracy: 0.7267 - val_loss: 0.6955 - val_accuracy: 0.7300\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.5407 - accuracy: 0.8059 - val_loss: 0.6086 - val_accuracy: 0.7525\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 1s 5ms/step - loss: 0.4057 - accuracy: 0.8675 - val_loss: 0.5807 - val_accuracy: 0.7719\n",
      "Epoch 5/20\n",
      "124/124 [==============================] - 1s 5ms/step - loss: 0.3042 - accuracy: 0.9073 - val_loss: 0.5736 - val_accuracy: 0.7756\n",
      "Epoch 6/20\n",
      "124/124 [==============================] - 1s 5ms/step - loss: 0.2272 - accuracy: 0.9380 - val_loss: 0.5865 - val_accuracy: 0.7844\n",
      "Epoch 7/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.1695 - accuracy: 0.9617 - val_loss: 0.6104 - val_accuracy: 0.7775\n",
      "Epoch 8/20\n",
      "124/124 [==============================] - 1s 5ms/step - loss: 0.1273 - accuracy: 0.9728 - val_loss: 0.6410 - val_accuracy: 0.7756\n",
      "Epoch 9/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0967 - accuracy: 0.9822 - val_loss: 0.6761 - val_accuracy: 0.7763\n",
      "Epoch 10/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0741 - accuracy: 0.9878 - val_loss: 0.7099 - val_accuracy: 0.7769\n",
      "Epoch 11/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0572 - accuracy: 0.9917 - val_loss: 0.7473 - val_accuracy: 0.7750\n",
      "Epoch 12/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0443 - accuracy: 0.9950 - val_loss: 0.7845 - val_accuracy: 0.7713\n",
      "Epoch 13/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0350 - accuracy: 0.9966 - val_loss: 0.8203 - val_accuracy: 0.7688\n",
      "Epoch 14/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0277 - accuracy: 0.9980 - val_loss: 0.8554 - val_accuracy: 0.7638\n",
      "Epoch 15/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0224 - accuracy: 0.9987 - val_loss: 0.8877 - val_accuracy: 0.7644\n",
      "Epoch 16/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0184 - accuracy: 0.9989 - val_loss: 0.9186 - val_accuracy: 0.7631\n",
      "Epoch 17/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0152 - accuracy: 0.9992 - val_loss: 0.9499 - val_accuracy: 0.7631\n",
      "Epoch 18/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0127 - accuracy: 0.9992 - val_loss: 0.9803 - val_accuracy: 0.7631\n",
      "Epoch 19/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0107 - accuracy: 0.9994 - val_loss: 1.0098 - val_accuracy: 0.7619\n",
      "Epoch 20/20\n",
      "124/124 [==============================] - 1s 4ms/step - loss: 0.0089 - accuracy: 0.9995 - val_loss: 1.0378 - val_accuracy: 0.7625\n"
     ]
    }
   ],
   "source": [
    "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
    "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n",
    "int_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 1s 4ms/step - loss: 0.5753 - accuracy: 0.7696\n"
     ]
    }
   ],
   "source": [
    "int_loss, int_accuracy = int_model.evaluate(int_test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7696250081062317\n",
      "0.5752687454223633\n"
     ]
    }
   ],
   "source": [
    "print(int_accuracy)\n",
    "print(int_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('Basic_text_classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "84d56173b818a73ea5c6160e46ed9fcb20e40f19d976849eaf324bfd63c96975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
