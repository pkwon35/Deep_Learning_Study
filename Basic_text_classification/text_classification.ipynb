{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic text classification\n",
    "\n",
    "## 참고 : https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "\n",
    "### [2022년 6월10일 ~ 6월18일]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItXfxkxvosLH"
   },
   "source": [
    "# 영화 리뷰를 사용한 텍스트 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 예시에서는 영화 리뷰(후기) 텍스트를 가져와서 이 후기는  \"긍정\" (positive) 또는 \"부정\" (negative)로 분류 해볼꺼에요.\n",
    "\n",
    "이 예제는 이진(binary, 두개로 분류) 즉 예측을 둘중 하나의 값으로 분류하는 문제입니다.  예) 긍정 / 부정, 사람이다 / 아니다, 등등 두개의 값으로 많이 사용되는 분류방법입니다.\n",
    "\n",
    "해당 예시에서는 tf.keras 모듈을 사용해서 해볼려고 합니다!\n",
    "\n",
    "그러기 위해서 일단 필요한 모듈을 설치 및 불러와 봐요~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg62Pmz3o83v"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요 모듈 설치 및 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해당 예시에서는 tensorflow  2.8.0 버전을 사용할것입니다.\n",
    "\n",
    "#### 아래 불러오기 함수가 실행이 안되면 아래 pip 코드 실행하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==\"2.8.0\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 GPU 지정    https://velog.io/@jaeha0725/%ED%8A%B9%EC%A0%95-GPU-%EC%A7%80%EC%A0%95-os.environCUDAVISIBLEDEVICES\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:40.138081Z",
     "iopub.status.busy": "2020-09-23T07:21:40.137445Z",
     "iopub.status.idle": "2020-09-23T07:21:46.446713Z",
     "shell.execute_reply": "2020-09-23T07:21:46.447247Z"
    },
    "id": "2ew7HTbPpCJH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# 필요 모듈 불러오기\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAsKG535pHep"
   },
   "source": [
    "## IMDB 데이터셋 다운로드\n",
    "\n",
    "IMDB 데이터셋은 텐서플로우에서 쉽게 불러와 사용할 수 있도록 미리 전처리 된 형태에서 제공하고 있어요.\n",
    "\n",
    "어떤 전처리를 해두었을까요?\n",
    "\n",
    "- string에서 숫자로 변환\n",
    "    - 우리가 보통 생각하기에 리뷰 데이터는 \"와 이 영화 진짜 개꿀잼\" 이런식으로 string 형식이겠죠. 하지만, 우리가 사용할 모델은 이런 string 형식이 input으로 들어오면 이해하지 못해요, 왜냐하면 모델은 \"숫자\"를 받아 프로세스 하기 때문이죠!\n",
    "\n",
    "- 숫자로 변환? 근데 어떻게??\n",
    "    - 주어진 string 형식의 정보를 변환해 모델에 넣어주는 방식은 여러가지가 있어요! 해당 예시에서 우리가 변환하는 방식은 간단해요!\n",
    "        - 특정 단어가 유일한 숫자값으로 mapping 되어있는 어휘사전으로 각 string을 숫자로 변화했어요!\n",
    "        - 조금 더 풀어서 얘기하면.. 일단 이번에 분석하고자하는 모든 데이터에 존재하는 모든 unique 단어를 찾아서 각각 다른 숫자를 지정해서 dictionary로 만들어주면 되요!\n",
    "             \n",
    "             {'I': 100,  'tsukino': 52006,  'nunnery': 52007,  'love': 16816, 'vani': 63951, 'woods': 1408, 'spiders': 16115, ... }\n",
    " \n",
    "        - 위와 같은 dictionary가 만들어지고, \"I love spiders\" 를 string -> 숫자로 변환한다면   100,16816,16115    이런식으로 바꿔서 데이터를 사용할 수 있겠죠?\n",
    "\n",
    "#### 아래에서 keras.datasets.imdb 데이터를 불러와봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:46.451714Z",
     "iopub.status.busy": "2020-09-23T07:21:46.451075Z",
     "iopub.status.idle": "2020-09-23T07:21:51.511011Z",
     "shell.execute_reply": "2020-09-23T07:21:51.510277Z"
    },
    "id": "zXXx5Oc3pOmN"
   },
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb   # keras에 있는 많은 datasets 예시에서 imdb 데이터 셋 object 불러오기\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)     \n",
    "# load_data 함수 사용해서 imdb 데이터의 train / test   데이터와 레이블 각각 불러오기     \n",
    "# num_words = 10000 을 설정하면 전체 데이터넷에서 많이 나타난 unique 한 단어 상위 10000개를 가져와요. 만약 100이라 하면 unique 100개만 가져오겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "word_index = imdb.get_word_index()\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_words 100 하면,   0~99로 각 단어를 변환한다. 즉 max는 99. (그렇다고 각 문서의 가장 큰 값이 99는 아니다. 해당 단어가 해당 리뷰에 없으면 더 작은값이 나옴)\n",
    "\n",
    "(train_data_100, train_labels_100), (test_data_100, test_labels_100) = imdb.load_data(num_words=100)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< num_words = 100 설정했을때 >\n",
      "각 문서에서 가장 큰 값\n",
      "[1, 99, 78, 13, 66, 40, 2, 2, 5, 2, 2, 21, 4, 2, 2, 8, 2, 14, 2, 2, 6, 2, 7, 2, 6, 2, 7, 2, 21, 12, 32, 2, 2, 2, 4, 2, 2, 81, 27, 2, 2, 2, 2, 2, 2, 38, 76, 2, 2, 11, 6, 2, 2, 8, 2, 4, 22, 14, 2, 7, 4, 2, 2, 2, 12, 17, 2, 2, 2, 2, 4, 58, 8, 2, 12, 2, 2, 62, 28, 2, 6, 2, 2]\n",
      "\n",
      "모든 문서에서 가장 큰 값\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(\"< num_words = 100 설정했을때 >\")\n",
    "print(\"각 문서에서 가장 큰 값\")\n",
    "print(max(train_data_100))\n",
    "print()\n",
    "print(\"모든 문서에서 가장 큰 값\")\n",
    "print(max(max(train_data_100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odr-KlzO-lkL"
   },
   "source": [
    "`num_words=10000`은 훈련 데이터에서 가장 많이 등장하는 상위 10,000개의 단어를 선택합니다.\n",
    "\n",
    "데이터 크기를 적당하게 유지하기 위해 드물게 등장하는 단어는 제외하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l50X3GfjpU4r"
   },
   "source": [
    "## 데이터 탐색\n",
    "\n",
    "이 데이터셋의 샘플은 전처리된 정수 배열입니다.\n",
    "\n",
    "    - 이 정수는 영화 리뷰에 나오는 단어를 나타냅니다.\n",
    "\n",
    "레이블(label)은 정수 0 또는 1입니다.\n",
    "\n",
    "    - 0은 부정적인 리뷰이고 1은 긍정적인 리뷰입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnKvHWW4-lkW"
   },
   "source": [
    "리뷰 텍스트는 어휘 사전의 특정 단어를 나타내는 정수로 변환되어 있습니다. 첫 번째 리뷰를 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:51.522456Z",
     "iopub.status.busy": "2020-09-23T07:21:51.521790Z",
     "iopub.status.idle": "2020-09-23T07:21:51.524067Z",
     "shell.execute_reply": "2020-09-23T07:21:51.524506Z"
    },
    "id": "QtTS4kpEpjbi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "\n",
      "unique 한 단어만 확인\n",
      "[   1    2    4    5    6    7    8    9   12   13   14   15   16   17\n",
      "   18   19   21   22   25   26   28   30   32   33   35   36   38   39\n",
      "   43   46   48   50   51   52   56   62   65   66   71   76   77   82\n",
      "   87   88   92   98  100  103  104  106  107  112  113  117  124  130\n",
      "  134  135  141  144  147  150  167  172  173  178  192  194  215  224\n",
      "  226  256  283  284  297  316  317  336  381  385  386  400  407  447\n",
      "  458  469  476  480  515  530  546  619  626  670  723  838  973 1029\n",
      " 1111 1247 1334 1385 1415 1622 1920 2025 2071 2223 3766 3785 3941 4468\n",
      " 4472 4536 4613 5244 5345 5535 5952 7486]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print()\n",
    "print(\"unique 한 단어만 확인\")\n",
    "print(np.unique(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 학습을 진행하기전, 해당 리뷰의 정체를 먼저 확인해봐요!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88584\n",
      "88584\n"
     ]
    }
   ],
   "source": [
    "# word_index 확인하기\n",
    "\n",
    "a=list(word_index.values())\n",
    "\n",
    "a.sort()\n",
    "\n",
    "print(max(a))\n",
    "print(len(a))\n",
    "\n",
    "# 확인해보니 연속적으로 0 ~ 88590 까지 있는 것은 아니다. 2개 정도 비어있는 것으로 확인됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 불러온 각 단어와 숫자가 매칭되어있는 word_index 딕셔너리를 활용해 변환을 해보겠습니다\n",
    "word_index = imdb.get_word_index()\n",
    "len(word_index)\n",
    "\n",
    "\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}   # 해당 예시에서는 word의 0 , 1 , 2 , 3 으로 <PAD> , <START> , <UNK> ,  <UNUSED>  추가해야합니다.\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])    # 기존 키값으로 단어가 들어있던 딘셔너리를    키값으로 숫자로    키값과 value값을 스위치 해주는 코드에요  예)  {\"I\":10 , \"love\" : 90}  -->  {10 : \"I\" , 90 : \"love\"}\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])    # 여기서 딕셔너리.get() 함수를 이해해보아요!!   dictionary.get(찾고자하는key값, 해당키값 없으면 돌려주는 value)  로 이해하면되요. \n",
    "                                                                        # 즉, reverse_word_index.get(i, '?') 에서   만약 reverse_word_index 안에 i값이 있으면, 해당 value를 돌려주지만, 만약 해당 i값이 존재하지 않으면 ? 를 돌려줍니다.\n",
    "\n",
    "\n",
    "decode_review(train_data[0])\n",
    "\n",
    "\n",
    "# 아래 결과에서 보이는 <UNK> 의 경우, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFP_XKVRp4_S"
   },
   "source": [
    "## 데이터 준비\n",
    "\n",
    "\n",
    "위에서 string을 숫자로 변환하였지만, 신경망에 주입하기 전 몇가지 변환을 더 해줘야합니다!\n",
    "\n",
    "\n",
    "- 리뷰(정수) 리스트형식의 배열은 신경망에 주입하기 전에 텐서로 변환해야합니다!\n",
    "\n",
    "해당 변환 방법에는 몇 가지가 있습니다:\n",
    "\n",
    "\n",
    "- ### 1) 원-핫 인코딩(one-hot encoding)은 정수 배열을 0과 1로 이루어진 벡터로 변환합니다. 예를 들어 배열 [3, 5]을 인덱스 3과 5만 1이고 나머지는 모두 0인 10,000차원 벡터로 변환할 수 있습니다. 그다음 실수 벡터 데이터를 다룰 수 있는 층-Dense 층-을 신경망의 첫 번째 층으로 사용합니다. 이 방법은 `num_words * num_reviews` 크기의 행렬이 필요하기 때문에 메모리를 많이 사용합니다.\n",
    "\n",
    "\n",
    "- ### 2) 정수 배열의 길이가 모두 같도록 패딩(padding)을 추가해 `max_length * num_reviews` 크기의 정수 텐서를 만듭니다. 이런 형태의 텐서를 다룰 수 있는 임베딩(embedding) 층을 신경망의 첫 번째 층으로 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "해당 예시에서는 두번째 방법인 \"정수 배열의 길이가 같도록 패딩(padding)을 추가하겠습니다!\n",
    "\n",
    "\n",
    "영화 리뷰의 길이가 같아야 하므로 [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) 함수를 사용해 길이를 맞추겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 450 129\n"
     ]
    }
   ],
   "source": [
    "# padding을 적용하기전 데이터의 길이를 비교해보겠습니다\n",
    "\n",
    "print(len(train_data[0]),len(train_data[10]),len(train_data[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:51.901032Z",
     "iopub.status.busy": "2020-09-23T07:21:51.900342Z",
     "iopub.status.idle": "2020-09-23T07:21:52.915360Z",
     "shell.execute_reply": "2020-09-23T07:21:52.914520Z"
    },
    "id": "2jQv-omsHurp"
   },
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],  ## 아까 word_index 에서 \"<PAD>\"\" 로 지정한 숫자, 0을 추가하도록 하겠습니다\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=256)\n",
    "\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VO5MBpyQdipD"
   },
   "source": [
    "샘플의 길이를 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:52.920484Z",
     "iopub.status.busy": "2020-09-23T07:21:52.919787Z",
     "iopub.status.idle": "2020-09-23T07:21:52.922925Z",
     "shell.execute_reply": "2020-09-23T07:21:52.922417Z"
    },
    "id": "USSSBnkE-lky"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256 256\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[0]),len(train_data[10]),len(train_data[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJoxZGyfjT5V"
   },
   "source": [
    "(패딩된) 첫 번째 리뷰 내용을 확인해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:52.927715Z",
     "iopub.status.busy": "2020-09-23T07:21:52.927036Z",
     "iopub.status.idle": "2020-09-23T07:21:52.929853Z",
     "shell.execute_reply": "2020-09-23T07:21:52.929354Z"
    },
    "id": "TG8X9cqi-lk9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "\n",
    "# 보이는 것과 같이 원래 218 였던 길아의 데이터에 0이 추가로 들어간 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLC02j2g-llC"
   },
   "source": [
    "## 모델 구성\n",
    "\n",
    "이제 데이터도 모델에 들어갈 준비가 완료됬습니다!!\n",
    "\n",
    "\n",
    "이제 데이터를 넣어줄 모델을 쌓아보죠.\n",
    "\n",
    "\n",
    "신경망은 층(layer)을 쌓아서 만듭니다. 이 구조에서는 두 가지를 결정해야 합니다:\n",
    "\n",
    "1) 모델에서 얼마나 많은 층을 사용할 것인가?\n",
    "2) 각 층에서 얼마나 많은 은닉 유닛(hidden unit)을 사용할 것인가?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- 이 예제의 입력 데이터는 단어 인덱스의 배열입니다.\n",
    "- 예측할 레이블은 0 또는 1입니다. (binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:52.935695Z",
     "iopub.status.busy": "2020-09-23T07:21:52.934804Z",
     "iopub.status.idle": "2020-09-23T07:21:54.626430Z",
     "shell.execute_reply": "2020-09-23T07:21:54.626871Z"
    },
    "id": "xpKOoWgu-llD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 23:46:37.620528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:37.642039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:37.643466: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:37.647310: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-19 23:46:37.649257: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:37.650767: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:37.652109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:38.770082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:38.771636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:38.773049: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-19 23:46:38.774534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:00:06.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "# 입력 크기는 영화 리뷰 데이터셋에 적용된 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "vocab_size = 10000\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))   # 해당 레이어를 지나면, 각 단어에서 context를 뽑아내는 과정을 거칩니다. \n",
    "model.add(keras.layers.GlobalAveragePooling1D())                         # Pooling에 대한 개념을 알고 있으면 좋을 것입니다.   https://gaussian37.github.io/dl-concept-global_average_pooling/\n",
    "                                                                        # 그냥 average pooling 1D 개녕 : https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/average-pooling-1d \n",
    "                                                                        # globalAveragePooling1D 개념 : https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/global-average-pooling-1d\n",
    "                                                                        \n",
    "model.add(keras.layers.Dense(16, activation='relu'))                    # relu 함수의 개념 : https://gooopy.tistory.com/55#:~:text=%EB%A0%90%EB%A3%A8%20%ED%95%A8%EC%88%98(Rectified%20Linear%20Unit%2C%20ReLU)&text=%EB%A0%90%EB%A3%A8%20%ED%95%A8%EC%88%98%EB%8A%94%20%EC%9A%B0%EB%A6%AC%20%EB%A7%90%EB%A1%9C,%EC%9D%84%20%EC%B0%A8%EB%8B%A8%ED%95%9C%EB%8B%A4%EB%8A%94%20%EC%9D%98%EB%AF%B8%EB%8B%A4.\n",
    "\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PbKQ6mucuKL"
   },
   "source": [
    "층을 순서대로 쌓아 분류기(classifier)를 만듭니다:\n",
    "\n",
    "1. 첫 번째 층은 `Embedding` 층입니다. 이 층은 정수로 인코딩된 단어를 입력 받고 각 단어 인덱스에 해당하는 임베딩 벡터를 찾습니다. 이 벡터는 모델이 훈련되면서 학습됩니다. 이 벡터는 출력 배열에 새로운 차원으로 추가됩니다. 최종 차원은 `(batch, sequence, embedding)`이 됩니다.\n",
    "\n",
    "2. 그다음 `GlobalAveragePooling1D` 층은 `sequence` 차원에 대해 평균을 계산하여 각 샘플에 대해 고정된 길이의 출력 벡터를 반환합니다. 이는 길이가 다른 입력을 다루는 가장 간단한 방법입니다. https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/global-average-pooling-1d\n",
    "\n",
    "3. 이 고정 길이의 출력 벡터는 16개의 은닉 유닛을 가진 완전 연결(fully-connected) 층(`Dense`)을 거칩니다.\n",
    "\n",
    "4. 마지막 층은 하나의 출력 노드(node)를 가진 완전 연결 층입니다. `sigmoid` 활성화 함수를 사용하여 0과 1 사이의 실수를 출력합니다. 이 값은 확률 또는 신뢰도를 나타냅니다.\n",
    "\n",
    "\n",
    "\n",
    "### 위에서 모델을 쌓아주었으니, 이제 해당 모델의 학습을 위해 컴파일 하겠습니다. (컴파일이란 모델을 학습시키기 위한 학습과정을 설정하는 단계)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4EqVWg4-llM"
   },
   "source": [
    "### 컴파일 함수로 \"손실 함수\"와 \"옵티마이저\" 설정하기\n",
    "\n",
    "모델이 훈련하려면 손실 함수(loss function)과 옵티마이저(optimizer)가 필요합니다. 이 예제는 이진 분류 문제이고 모델이 확률을 출력하므로(출력층의 유닛이 하나이고 `sigmoid` 활성화 함수를 사용합니다), `binary_crossentropy` 손실 함수를 사용하겠습니다.\n",
    "\n",
    "binary_crossentropy : 확률 분포 간의 거리를 측정하는 함수. 정답인 타깃 분포와 예측 분포 사이의 거리를 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:54.639995Z",
     "iopub.status.busy": "2020-09-23T07:21:54.639227Z",
     "iopub.status.idle": "2020-09-23T07:21:54.647130Z",
     "shell.execute_reply": "2020-09-23T07:21:54.646492Z"
    },
    "id": "Mr0GP-cQ-llN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCWYwkug-llQ"
   },
   "source": [
    "## 검증 세트 만들기\n",
    "\n",
    "모델링을 할때 주어진 데이터를 3가지로 나눠서 보통 진행합니다.\n",
    "\n",
    "- 학습 (train)\n",
    "- 검증 (validation)\n",
    "- 테스트 (test)\n",
    "\n",
    "\n",
    "모델을 훈련하면서 \"학습\" 데이터와 \"검증\" 데이터를 사용합니다. 다만, 모델 학습시 가중치에 영향을 주는 데이터는 오직 \"학습\" 데이터 입니다. 검증 데이터는 단순히 모델이 훈련되면서 해당 모델에 아무런 영향은 주지 않은 검증데이터의 정확도를 확인하기위해 활용됩니다.\n",
    "\n",
    "- 중요) 모델 \"학습\" 데이터를 사용해 학습하며 \"검증\" 데이터로 정확도를 확인하지만, 얼마나 검증 데이터의 정확도를 잘 측정하는지는 학습과정에 아무런 영향을 주지 않습니다. \n",
    "\n",
    "- 그럼 검증데이터와 테스트 데이터는 비슷한 역활을 하는 것일까요?\n",
    "    - 훈련 데이터만을 사용하여 모델을 개발하고 튜닝하는 것이 목표입니다! 이후 테스트 세트를 사용해서 딱 한 번만 정확도를 평가합니다\n",
    "\n",
    "\n",
    "\n",
    "### 원본 훈련 데이터에서 10,000개의 샘플을 떼어내어 검증 데이터 (validation set)를 만들어봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:54.651865Z",
     "iopub.status.busy": "2020-09-23T07:21:54.651213Z",
     "iopub.status.idle": "2020-09-23T07:21:54.653218Z",
     "shell.execute_reply": "2020-09-23T07:21:54.653608Z"
    },
    "id": "-NpcXY9--llS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "x_val = train_data[:10000]\n",
    "partial_x_train = train_data[10000:]\n",
    "y_val = train_labels[:10000]\n",
    "partial_y_train = train_labels[10000:]\n",
    "\n",
    "print(len(x_val))  # 검증 10000개\n",
    "print(len(partial_x_train))  # 학습 15000개\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35jv_fzP-llU"
   },
   "source": [
    "## 모델 훈련\n",
    "### 참고 : https://m.blog.naver.com/qbxlvnf11/221449297033\n",
    "모델을 훈련하기위해 설정해줘야하는 여러가지 옵션이 있습니다!\n",
    "\n",
    "\n",
    "\n",
    "- epochs\n",
    "    - 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것을 말함. 즉, 전체 데이터 셋에 대해 한 번 학습을 완료한 상태\n",
    "    - 우리는 모델을 만들 때 적절한 epoch 값을 설정해야만 underfitting과 overfitting을 방지할 수 있습니다.\n",
    "    - epoch 값이 너무 작다면 underfitting이 너무 크다면 overfitting이 발생할 확률이 높은 것이죠.\n",
    "\n",
    "\n",
    "- batch_size\n",
    "    - batch size는 한 번의 batch마다 주는 데이터 샘플의 size. 여기서 batch(보통 mini-batch라고 표현)는 나눠진 데이터 셋을 뜻하며 iteration는 epoch를 나누어서 실행하는 횟수라고 생각하면 됨\n",
    "    - 메모리의 한계와 속도 저하 때문에 대부분의 경우에는 한 번의 epoch에서 모든 데이터를 한꺼번에 집어넣을 수는 없습니다. 그래서 데이터를 나누어서\n",
    "\n",
    "\n",
    "![image](https://mblogthumb-phinf.pstatic.net/MjAxOTAxMjNfMjU4/MDAxNTQ4MjM1Nzg3NTA2.UtvnGsckZhLHOPPOBWH841IWsZFzNcgwZvYKi2nxImEg.CdtqIxOjWeBo4eNBD2pXu5uwYGa3ZVUr8WZvtldArtYg.PNG.qbxlvnf11/20190123_182720.png?type=w800)\n",
    "\n",
    "\n",
    "- verbose\n",
    "    - 학습시 프린트되는 과정을 설정\n",
    "    - verbose: Integer. 0, 1, or 2. \n",
    "    - 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "\n",
    "\n",
    "- validation_date\n",
    "    - 설정시 학습시 검증 데이터를 모델로 검증한 결과를 프린트 같이 해줌\n",
    "\n",
    "\n",
    "등등 여러가지가 있습니다. \n",
    "\n",
    "이 모델을 512개의 샘플로 이루어진 미니배치(mini-batch)에서 40번의 에포크(epoch) 동안 훈련합니다. `x_train`과 `y_train` 텐서에 있는 모든 샘플에 대해 40번 반복한다는 뜻입니다. 훈련하는 동안 10,000개의 검증 세트에서 모델의 손실과 정확도를 모니터링합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:21:54.658876Z",
     "iopub.status.busy": "2020-09-23T07:21:54.658250Z",
     "iopub.status.idle": "2020-09-23T07:22:08.874375Z",
     "shell.execute_reply": "2020-09-23T07:22:08.874803Z"
    },
    "id": "tXSGrjWZ-llW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "23/30 [======================>.......] - ETA: 0s - loss: 0.6925 - accuracy: 0.5195"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 23:46:44.559110: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 4s 15ms/step - loss: 0.6922 - accuracy: 0.5255 - val_loss: 0.6904 - val_accuracy: 0.5763\n",
      "Epoch 2/40\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6873 - accuracy: 0.6134 - val_loss: 0.6832 - val_accuracy: 0.6535\n",
      "Epoch 3/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6765 - accuracy: 0.6779 - val_loss: 0.6693 - val_accuracy: 0.7057\n",
      "Epoch 4/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.6573 - accuracy: 0.7238 - val_loss: 0.6470 - val_accuracy: 0.7492\n",
      "Epoch 5/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.6283 - accuracy: 0.7542 - val_loss: 0.6161 - val_accuracy: 0.7683\n",
      "Epoch 6/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5910 - accuracy: 0.7946 - val_loss: 0.5787 - val_accuracy: 0.7936\n",
      "Epoch 7/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.5461 - accuracy: 0.8249 - val_loss: 0.5344 - val_accuracy: 0.8201\n",
      "Epoch 8/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4969 - accuracy: 0.8490 - val_loss: 0.4908 - val_accuracy: 0.8361\n",
      "Epoch 9/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.4501 - accuracy: 0.8617 - val_loss: 0.4516 - val_accuracy: 0.8469\n",
      "Epoch 10/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.4087 - accuracy: 0.8753 - val_loss: 0.4185 - val_accuracy: 0.8538\n",
      "Epoch 11/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3727 - accuracy: 0.8835 - val_loss: 0.3917 - val_accuracy: 0.8595\n",
      "Epoch 12/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3431 - accuracy: 0.8913 - val_loss: 0.3697 - val_accuracy: 0.8635\n",
      "Epoch 13/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.3180 - accuracy: 0.8973 - val_loss: 0.3518 - val_accuracy: 0.8703\n",
      "Epoch 14/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2959 - accuracy: 0.9021 - val_loss: 0.3375 - val_accuracy: 0.8732\n",
      "Epoch 15/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2774 - accuracy: 0.9067 - val_loss: 0.3261 - val_accuracy: 0.8752\n",
      "Epoch 16/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2612 - accuracy: 0.9111 - val_loss: 0.3171 - val_accuracy: 0.8787\n",
      "Epoch 17/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2465 - accuracy: 0.9163 - val_loss: 0.3095 - val_accuracy: 0.8794\n",
      "Epoch 18/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.2335 - accuracy: 0.9215 - val_loss: 0.3034 - val_accuracy: 0.8810\n",
      "Epoch 19/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2216 - accuracy: 0.9253 - val_loss: 0.2989 - val_accuracy: 0.8821\n",
      "Epoch 20/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2108 - accuracy: 0.9286 - val_loss: 0.2955 - val_accuracy: 0.8810\n",
      "Epoch 21/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.2011 - accuracy: 0.9316 - val_loss: 0.2915 - val_accuracy: 0.8833\n",
      "Epoch 22/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1916 - accuracy: 0.9370 - val_loss: 0.2897 - val_accuracy: 0.8836\n",
      "Epoch 23/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1834 - accuracy: 0.9398 - val_loss: 0.2874 - val_accuracy: 0.8837\n",
      "Epoch 24/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1759 - accuracy: 0.9419 - val_loss: 0.2864 - val_accuracy: 0.8841\n",
      "Epoch 25/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1680 - accuracy: 0.9474 - val_loss: 0.2848 - val_accuracy: 0.8855\n",
      "Epoch 26/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1606 - accuracy: 0.9502 - val_loss: 0.2846 - val_accuracy: 0.8841\n",
      "Epoch 27/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1544 - accuracy: 0.9532 - val_loss: 0.2843 - val_accuracy: 0.8852\n",
      "Epoch 28/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1478 - accuracy: 0.9546 - val_loss: 0.2856 - val_accuracy: 0.8857\n",
      "Epoch 29/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1420 - accuracy: 0.9569 - val_loss: 0.2857 - val_accuracy: 0.8859\n",
      "Epoch 30/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1365 - accuracy: 0.9593 - val_loss: 0.2869 - val_accuracy: 0.8845\n",
      "Epoch 31/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9617 - val_loss: 0.2877 - val_accuracy: 0.8857\n",
      "Epoch 32/40\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 0.1259 - accuracy: 0.9639 - val_loss: 0.2894 - val_accuracy: 0.8853\n",
      "Epoch 33/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1211 - accuracy: 0.9661 - val_loss: 0.2918 - val_accuracy: 0.8859\n",
      "Epoch 34/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1164 - accuracy: 0.9672 - val_loss: 0.2961 - val_accuracy: 0.8833\n",
      "Epoch 35/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1120 - accuracy: 0.9695 - val_loss: 0.2964 - val_accuracy: 0.8846\n",
      "Epoch 36/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.1078 - accuracy: 0.9715 - val_loss: 0.2983 - val_accuracy: 0.8849\n",
      "Epoch 37/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1044 - accuracy: 0.9726 - val_loss: 0.3020 - val_accuracy: 0.8834\n",
      "Epoch 38/40\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1000 - accuracy: 0.9745 - val_loss: 0.3038 - val_accuracy: 0.8843\n",
      "Epoch 39/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0964 - accuracy: 0.9759 - val_loss: 0.3065 - val_accuracy: 0.8841\n",
      "Epoch 40/40\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 0.0933 - accuracy: 0.9773 - val_loss: 0.3097 - val_accuracy: 0.8837\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=40,  # 전체 데이터를 40번 사용해서 학습을 거치는 것입니다\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EEGuDVuzb5r"
   },
   "source": [
    "## 모델 평가\n",
    "\n",
    "모델의 성능을 확인해 보죠. 두 개의 값이 반환됩니다. 손실(오차를 나타내는 숫자이므로 낮을수록 좋습니다)과 정확도입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  591,  202, ...,    0,    0,    0],\n",
       "       [   6,  176,    7, ...,  125,    4, 3077],\n",
       "       [  57, 4893,    5, ...,    9,   57,  975],\n",
       "       ...,\n",
       "       [   1,   13, 1408, ...,    0,    0,    0],\n",
       "       [   1,   11,  119, ...,    0,    0,    0],\n",
       "       [   1,    6,   52, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:22:08.879756Z",
     "iopub.status.busy": "2020-09-23T07:22:08.879054Z",
     "iopub.status.idle": "2020-09-23T07:22:10.410627Z",
     "shell.execute_reply": "2020-09-23T07:22:10.411068Z"
    },
    "id": "zOMKywn4zReN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 1s - loss: 0.3313 - accuracy: 0.8725 - 1s/epoch - 2ms/step\n",
      "[0.33133482933044434, 0.8724799752235413]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data,  test_labels, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KggXVeL-llZ"
   },
   "source": [
    "## 정확도와 손실 그래프 그리기\n",
    "\n",
    "`model.fit()`은 `History` 객체를 반환합니다. 여기에는 훈련하는 동안 일어난 모든 정보가 담긴 딕셔너리(dictionary)가 들어 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:22:10.415929Z",
     "iopub.status.busy": "2020-09-23T07:22:10.415051Z",
     "iopub.status.idle": "2020-09-23T07:22:10.417656Z",
     "shell.execute_reply": "2020-09-23T07:22:10.418079Z"
    },
    "id": "VcvSXvhp-llb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습하며 각 epoch에서의 결과를 쌓아둔 것이다\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6921915411949158, 0.6873483061790466, 0.6764644980430603, 0.6572516560554504, 0.6282529234886169, 0.590996265411377, 0.5460619926452637, 0.4968883693218231, 0.450130820274353, 0.40866267681121826, 0.3727050721645355, 0.3431067168712616, 0.31800270080566406, 0.2959008812904358, 0.27744928002357483, 0.2611619830131531, 0.24649299681186676, 0.23350396752357483, 0.22164268791675568, 0.21083872020244598, 0.2011403739452362, 0.19160403311252594, 0.18344441056251526, 0.17590267956256866, 0.1680263876914978, 0.16061191260814667, 0.15440498292446136, 0.1477610170841217, 0.1419777274131775, 0.13645975291728973, 0.1308193802833557, 0.12587353587150574, 0.12111834436655045, 0.11638690531253815, 0.11199944466352463, 0.10776693373918533, 0.10437742620706558, 0.10001237690448761, 0.09643866866827011, 0.09333091974258423]\n",
      "\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(history_dict['loss'])\n",
    "print()\n",
    "print(len(history_dict['loss']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRKsqL40-lle"
   },
   "source": [
    "네 개의 항목이 있습니다. 훈련과 검증 단계에서 모니터링하는 지표들입니다. 훈련 손실과 검증 손실을 그래프로 그려 보고, 훈련 정확도와 검증 정확도도 그래프로 그려서 비교해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:22:10.424038Z",
     "iopub.status.busy": "2020-09-23T07:22:10.423349Z",
     "iopub.status.idle": "2020-09-23T07:22:10.732376Z",
     "shell.execute_reply": "2020-09-23T07:22:10.732967Z"
    },
    "id": "nGoYf2Js-lle"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(1, 41)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "print(epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \"bo\"는 \"파란색 점\"입니다\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b는 \"파란 실선\"입니다\n",
    "\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T07:22:10.749404Z",
     "iopub.status.busy": "2020-09-23T07:22:10.742226Z",
     "iopub.status.idle": "2020-09-23T07:22:10.887415Z",
     "shell.execute_reply": "2020-09-23T07:22:10.888043Z"
    },
    "id": "6hXx-xOv-llh"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()   # 그림을 초기화합니다\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.tensorflow.org/tutorials/keras/text_classification#exercise_multi-class_classification_on_stack_overflow_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 예시 : multi-class (다중클래스) 분류 on Stack Overflow questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 진행한 binary 예시를 참고해서 stack overflow 질문 자료를 다중클래스 학습에 적용해봐요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download dataset\n",
    "# %%capture\n",
    "# !gzip -d /home/jkwon/tasks/Basic_text_classification/stack_overflow_16k.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !tar -xvf /home/jkwon/tasks/Basic_text_classification/stack_overflow_16k.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you will work with contains several thousand questions extracted from the much larger public Stack Overflow dataset on BigQuery, which contains more than 17 million posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading the dataset, you will find it has a similar directory structure to the IMDB dataset you worked with previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고1 : https://www.gcptutorials.com/post/how-to-use-text_dataset_from_directory-in-tensorflow\n",
    "# 참고2 : https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\n",
    "# 참고3 : https://www.tensorflow.org/tutorials/load_data/text\n",
    "batch_size=52\n",
    "seed=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n",
      "Using 6400 files for training.\n",
      "Found 8000 files belonging to 4 classes.\n",
      "Using 1600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# 학습 데이터셋 만들기\n",
    "raw_train_ds=utils.text_dataset_from_directory(\n",
    "    \"/home/jkwon/tasks/Basic_text_classification/train\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    class_names=None,\n",
    "    batch_size=batch_size,\n",
    "    max_length=None,\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    follow_links=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 검증 데이터셋 만들기\n",
    "raw_val_ds = utils.text_dataset_from_directory(\n",
    "    \"/home/jkwon/tasks/Basic_text_classification/train\",\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 corresponds to csharp\n",
      "Label 1 corresponds to java\n",
      "Label 2 corresponds to javascript\n",
      "Label 3 corresponds to python\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(raw_train_ds.class_names):\n",
    "    print(\"Label\", i, \"corresponds to\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review b\"how to generate a unique 4-digit string i'm looking for a way to generate a (fairly) unique (non auto-incrementing) 4-digit string using the numbers 0 - 9 for each digit using blank.  i can validate uniqueness and generate another number if a dup is found.  i had thought about basing the number somehow on the datetime object's ticks property but am having a difficult time putting the pieces together...any thoughts or expertise would be much appreciated.\\n\"\n",
      "Label 0\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "  for i in range(1):\n",
    "    print(\"Review\", text_batch.numpy()[i])\n",
    "    print(\"Label\", label_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터셋 만들기\n",
    "raw_test_ds = utils.text_dataset_from_directory(\n",
    "    \"/home/jkwon/tasks/Basic_text_classification/test\",\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 과정에서 데이터셋을 각 용도에 따라 나누어주었습니다. 하지만, 모델에 넣기위해 아직 남은 과정이 있습니다!\n",
    "\n",
    "1) standardization\n",
    "    - 문자를 전처리하는 과정\n",
    "        - remove punctuation / html elements\n",
    "\n",
    "2) tokenization\n",
    "    - 각 단어를 토큰화, 특정 단어에서 같은 의미지만 다른 형식으로 변형된 단어를 동일시 취급하여 unique 한 단어 수를 줄이기 위한 과정\n",
    "\n",
    "3) vectorization\n",
    "    - 각 단어를 숫자로 변형 및 벡터화\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위 과정은 'tf.keras.layers.TextVectorization' 함수로 가능합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어의 빈도를 사용한 벡터 생성\n",
    "\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "binary_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='binary') # output_mode 에서 'binary' 를 사용하면 bag-of-words 모델을 만드는 것이다 : Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a text-only dataset (without labels), then call `TextVectorization.adapt`.\n",
    "train_text = raw_train_ds.map(lambda text, labels: text)\n",
    "\n",
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return binary_vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question tf.Tensor(b'\"blank not working on any internet explorer version i am a newbie programmer but i was assign a project and i am having trouble getting this code to work on internet explorer. it works on firefox, chrome and safari.  in the developer console in ie, it keeps saying s1 is undefined.  i found the blank code on stack\\'s overflow.  basically what i want is if the user answer yes to any of the question, it will redirect them to page, and if they answer no to all the questions it will redirect them to a different page...        &lt;div&gt;.          &lt;label&gt; a. are you a programmer?&lt;/label&gt;.          &lt;select id=\"\"s1\"\" name=\"\"menu\"\" onchange=\"\"gotopage(this)\"\"&gt;.            &lt;option value=\"\"#\"\"&gt;select&lt;/option&gt;.            &lt;option value=\"\"yes\"\"&gt;yes&lt;/option&gt;.            &lt;option value=\"\"no\"\"&gt;no&lt;/option&gt;.          &lt;/select&gt;.        &lt;/div&gt;.        &lt;div&gt;.          &lt;label&gt; b. are you over 18?&lt;/label&gt;.          &lt;select id=\"\"s2\"\" name=\"\"menu\"\" onchange=\"\"gotopage(this)\"\"&gt;.            &lt;option value=\"\"#\"\"&gt;select&lt;/option&gt;.            &lt;option value=\"\"yes\"\"&gt;yes&lt;/option&gt;.            &lt;option value=\"\"no\"\"&gt;no&lt;/option&gt;.          &lt;/select&gt;.        &lt;/div&gt;.        &lt;div&gt;.          &lt;label&gt; c. do you like apples?&lt;/label&gt;            .          &lt;select id=\"\"s3\"\" name=\"\"menu\"\" onchange=\"\"gotopage(this)\"\"&gt;.            &lt;option value=\"\"#\"\"&gt;select&lt;/option&gt;.            &lt;option value=\"\"yes\"\"&gt;yes&lt;/option&gt;.            &lt;option value=\"\"no\"\"&gt;no&lt;/option&gt;.          &lt;/select&gt;.        &lt;/div&gt;    ...&lt;script&gt;..  function gotopage(){..  if(s1.value == \"\"yes\"\" || s2.value == \"\"yes\"\" || s3.value == \"\"yes\"\") {.    window.location = \"\"http://www.yahoo.com\"\";.  } else if(s1.value == \"\"no\"\" &amp;&amp; s2.value == \"\"no\"\" &amp;&amp; s3.value == \"\"no\"\") {.    window.location = \"\"http://www.google.com\"\";.  } .}..&lt;/script&gt;...i also tried the code on this page get selected value of option with multiple dropdown menus using blank. can someone direct me to the correct solution or give me a hint? sorry for the bad coding.\"\\n', shape=(), dtype=string)\n",
      "\n",
      "Label tf.Tensor(2, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve a batch (of 32 reviews and labels) from the dataset.\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_question, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Question\", first_question)\n",
    "print()\n",
    "print(\"Label\", first_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'binary' vectorized question: tf.Tensor([[1. 1. 1. ... 0. 0. 0.]], shape=(1, 10000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"'binary' vectorized question:\",\n",
    "      binary_vectorize_text(first_question, first_label)[0])\n",
    "\n",
    "## 아래는 10000개의 단어에 대해서 나타난 횟수를 보여주는 bag of words 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"1289 ---> \", int_vectorize_layer.get_vocabulary()[1289])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_ds = raw_train_ds.map(binary_vectorize_text)\n",
    "binary_val_ds = raw_val_ds.map(binary_vectorize_text)\n",
    "binary_test_ds = raw_test_ds.map(binary_vectorize_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_ds = configure_dataset(binary_train_ds)\n",
    "binary_val_ds = configure_dataset(binary_val_ds)\n",
    "binary_test_ds = configure_dataset(binary_test_ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "124/124 [==============================] - 2s 12ms/step - loss: 1.1750 - accuracy: 0.6202 - val_loss: 1.0059 - val_accuracy: 0.7281\n",
      "Epoch 2/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.8563 - accuracy: 0.8161 - val_loss: 0.8456 - val_accuracy: 0.7563\n",
      "Epoch 3/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.7006 - accuracy: 0.8527 - val_loss: 0.7569 - val_accuracy: 0.7738\n",
      "Epoch 4/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.6020 - accuracy: 0.8809 - val_loss: 0.6999 - val_accuracy: 0.7781\n",
      "Epoch 5/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.8989 - val_loss: 0.6599 - val_accuracy: 0.7900\n",
      "Epoch 6/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.4780 - accuracy: 0.9128 - val_loss: 0.6304 - val_accuracy: 0.7912\n",
      "Epoch 7/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.9236 - val_loss: 0.6077 - val_accuracy: 0.7994\n",
      "Epoch 8/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.9297 - val_loss: 0.5899 - val_accuracy: 0.8031\n",
      "Epoch 9/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.3693 - accuracy: 0.9364 - val_loss: 0.5756 - val_accuracy: 0.8006\n",
      "Epoch 10/10\n",
      "124/124 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.9414 - val_loss: 0.5640 - val_accuracy: 0.8006\n"
     ]
    }
   ],
   "source": [
    "binary_model = tf.keras.Sequential([layers.Dense(4)])\n",
    "\n",
    "binary_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = binary_model.fit(\n",
    "    binary_train_ds, validation_data=binary_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 2s 10ms/step - loss: 0.5499 - accuracy: 0.8075\n"
     ]
    }
   ],
   "source": [
    "binary_loss, binary_accuracy = binary_model.evaluate(binary_test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vectorized layer를 사용한 1D ConvNet 결과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocab_size, num_labels):\n",
    "  model = tf.keras.Sequential([\n",
    "      layers.Embedding(vocab_size, 64, mask_zero=True),\n",
    "      layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "      layers.GlobalMaxPooling1D(),\n",
    "      layers.Dense(num_labels)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'int' vectorized question: tf.Tensor(\n",
      "[[  16   23  158   37   75 1257 2157  456    3   34    5 1618 2044   26\n",
      "     3  120  604    5  262    8    3   34  230  584  151   13   28    4\n",
      "   138   37 1257 2157   11  182   37 1529  841    8 2526    7    2 2008\n",
      "   335    7  434   11 1097  861 1372    6  436    3  225    2   16   28\n",
      "    37 7217 2361  637   54    3   44    6   10    2  104  216  628    4\n",
      "    75    9    2  159   11   72 1644  184    4  215    8   10  208  216\n",
      "   136    4   73    2  775   11   72 1644  184    4    5  178  215  530\n",
      "  3624    5   61   64    5    1 2366    1    1    1  599 7010  599    1\n",
      "   599    1 3867  530  530 3624  113   61   64  317    1 2366    1    1\n",
      "     1  599 7010  599    1  599    1 3867  530  530 3624  142   41   64\n",
      "    48    1 2366    1    1    1  599 7010  599    1  599    1 3867  530\n",
      "   895   38    1    1  628    1  628    1  628 2705    1   49    1  136\n",
      "   144    1  136  144    1  136 2705 7778    1  174  145    2   28   37\n",
      "    13  215   40  491   51    9  536   22  260 1115 4553   47   16   35\n",
      "   289 2677   74    4    2  247  290   46  381   74    5 2378  784   12\n",
      "     2  901  907    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0]], shape=(1, 250), dtype=int64)\n",
      "313 --->  long\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "# 숫자 형식으로 변환해주는 과정\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "int_vectorize_layer = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "int_vectorize_layer.adapt(train_text)\n",
    "\n",
    "def int_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return int_vectorize_layer(text), label\n",
    "\n",
    "\n",
    "print(\"'int' vectorized question:\",\n",
    "      int_vectorize_text(first_question, first_label)[0])\n",
    "\n",
    "## 동일 250 길이의 벡터를 위해 padding 추가되어 있음\n",
    "\n",
    "print(\"313 ---> \", int_vectorize_layer.get_vocabulary()[313])\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))\n",
    "int_train_ds = raw_train_ds.map(int_vectorize_text)\n",
    "int_val_ds = raw_val_ds.map(int_vectorize_text)\n",
    "int_test_ds = raw_test_ds.map(int_vectorize_text)\n",
    "int_train_ds = configure_dataset(int_train_ds)\n",
    "int_val_ds = configure_dataset(int_val_ds)\n",
    "int_test_ds = configure_dataset(int_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-19 23:47:09.087032: E tensorflow/stream_executor/cuda/cuda_dnn.cc:361] Loaded runtime CuDNN library: 8.0.4 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2022-06-19 23:47:09.088351: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at conv_ops.cc:1120 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_2/conv1d/Conv1D' defined at (most recent call last):\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_27046/2671505958.py\", line 7, in <module>\n      history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/layers/convolutional.py\", line 248, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/layers/convolutional.py\", line 240, in convolution_op\n      name=self.__class__.__name__)\nNode: 'sequential_2/conv1d/Conv1D'\nDNN library is not found.\n\t [[{{node sequential_2/conv1d/Conv1D}}]] [Op:__inference_train_function_22383]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27046/2671505958.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     metrics=['accuracy'])\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_2/conv1d/Conv1D' defined at (most recent call last):\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/traitlets/config/application.py\", line 976, in launch_instance\n      app.start()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2976, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3258, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_27046/2671505958.py\", line 7, in <module>\n      history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/sequential.py\", line 374, in call\n      return super(Sequential, self).call(inputs, training=training, mask=mask)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/layers/convolutional.py\", line 248, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/jkwon/anaconda3/envs/Basic_text_classification/lib/python3.7/site-packages/keras/layers/convolutional.py\", line 240, in convolution_op\n      name=self.__class__.__name__)\nNode: 'sequential_2/conv1d/Conv1D'\nDNN library is not found.\n\t [[{{node sequential_2/conv1d/Conv1D}}]] [Op:__inference_train_function_22383]"
     ]
    }
   ],
   "source": [
    "# `vocab_size` is `VOCAB_SIZE + 1` since `0` is used additionally for padding.\n",
    "int_model = create_model(vocab_size=VOCAB_SIZE + 1, num_labels=4)\n",
    "int_model.compile(\n",
    "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "history = int_model.fit(int_train_ds, validation_data=int_val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting protobuf==3.20.1\n",
      "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.4\n",
      "    Uninstalling protobuf-3.19.4:\n",
      "      Successfully uninstalled protobuf-3.19.4\n",
      "Successfully installed protobuf-3.20.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install protobuf==\"3.20.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "text_classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('Basic_text_classification')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "84d56173b818a73ea5c6160e46ed9fcb20e40f19d976849eaf324bfd63c96975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
